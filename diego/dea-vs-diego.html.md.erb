---
title: DEA / Diego Differences
---

This topic describes components and functions that changed significantly when Cloud Foundry&reg; migrated to Diego architecture for version 1.5. This information should help people who are familiar with Cloud Foundry's DEA-based architecture and want to learn what has changed under Diego and how its new or changed components work.

##<a id='design'></a>High-Level Design

The Diego redesign piggybacked off of migrating code from Ruby to Go. Rewriting components in the new language offered the opportunity to redesign them and incorporate lessons learned.

In a pre-Diego Cloud Foundry deployment, the Cloud Controller scheduled and managed applications on [Droplet Execution Agents](../architecture/execution-agent.html) (DEAs) while the [Health Manager (HM9000)](../architecture/index.html#hm9k) kept them running. The Diego system assumes application scheduling and management responsibility from the Cloud Controller, replacing the DEAs and Health Manager.

DEA architecture made no distinction between machine jobs that run once and jobs that run continuously. Diego recognizes the difference and uses it to allocate jobs to VMs more efficiently, replacing the [DEA Placement Algorithm](../architecture/cloud-controller.html#dea-placement) with the [Diego Auction](./diego-auction.html).

The Cloud Foundry migration to Diego architecture includes other changes and renamings. The table below summarizes the main differences, and the following sections describe pre-Diego components and their newer analogs. 

##<a id='table'></a>DEA / Diego Differences Summary

<table><tbody>
<tr>
<th>Function</th>
<th>DEA architecture</th>
<th>Diego architecture</th>
<th>&Delta; notes</th></tr>
<tr>
<td>Source code language</td>
<td>Ruby</td>
<td>Go</td>
<td></td></tr>
<tr>
<td>Mid-level manager on each VM that runs apps as directed and communicates &ldquo;heartbeat&rdquo;, application status and container location, and other messages.</td>
<td>DEA</td>
<td>Diego Cell</td>
<td>Runs on each VM that hosts apps, as opposed to special-purpose component VMs.</td></tr>
<tr>
<td>Low-level manager and API protocol on each VM for creating, configuring, destroying, monitoring, and addressing application containers.</td>
<td>Warden</td>
<td>Garden and platform-specific Garden backends</td>
<td>Warden is Linux-only, Garden backend is abstracted to support multiple OSs </td></tr>
<tr>
<td>Process that monitors application instances to ensure that enough are running.</td>
<td>Health Manager (HM9000)</td>
<td>nSync (syncs between Cloud Controller and Diego) and Converger&nbsp;(syncs within Diego)</td>
<td></td></tr>
<tr><td>Algorithm used to allocate processes to VMs</td>
<td>DEA Placement Algorithm</td>
<td>Diego Auction</td>
<td>Diego Auction distinguishes between Task and Long-Running Process (LRP) job types.</td></tr>
<tr><td>Internal communication between components</td>
<td>NATS Message Bus</td>
<td>Bulletin Board System (BBS) and Consul via http/s, and NATS</td>
<td>BBS stores most runtime data; Consul stores control data.</td></tr>
<tr><td>CF component that routes external traffic to/from applications</td>
<td>Router</td>
<td>Gorouter</td>
<td></td></tr></tbody></table>

##<a id='component'></a>Changed Components and Functions

### <a id='dea'></a>DEA &rarr; Diego Cell

The pre-Diego [Droplet Execution Agent](../architecture/execution-agent.html) (DEA) component managed application
instances, tracked started instances, and broadcast state messages. These functions are now performed by the [Diego cell](../architecture/index.html#diego-cell).

### <a id='warden'></a> Warden &rarr; Garden

Pre-Diego application instances lived inside [Warden](../architecture/warden.html) containers, which are analogous to [Garden](../diego/diego-components.html#cell-components) containers in Diego architecture.
Containerization ensures that application instances run in isolation, get their
fair share of resources, and are protected from "noisy neighbors" - other applications running on the same machine.

For network security, pre-Diego releases of Cloud Foundry supported `allow` and `deny`rules that governed outbound traffic from all Warden containers running on the same DEA. Newer releases use container-specific Application Security Groups (ASGs) to restrict traffic at a more granular level. Cloud Foundry recommends using ASGs exclusively, but when a pre-Diego deployment defined both Warden rules and ASGs, they were evaluated in a strict priority order.

Pre-Diego Cloud Foundry returned an allow, deny, or reject result for the first rule that matched the outbound traffic request parameters, and did not evaluate any lower-priority rules.
Cloud Foundry evaluated the network traffic rules for an application in the following order:

1. **Security Groups**: The rules described by the Default Staging set, the
Default Running set, and all security groups bound to the space.
1. **Warden ALLOW rules**: Any Warden Server configuration `allow` rules.
Set Warden Server configuration rules in the Droplet Execution Agent (DEA)
configuration section of your deployment manifest.
1. **Warden DENY rules**: Any Warden Server configuration `deny` rules.
Set Warden Server configuration rules in the DEA configuration section of your
deployment manifest.
1. **Hard-coded REJECT rule**: Cloud Foundry returns a `reject` result for all
outbound traffic from a container if not allowed by a higher-priority rule.

### <a id='hm9k'></a>Health Manager (HM9000) &rarr; nsync, Converger, and Cell Rep

The function of the Health Monitor (HM9000) component in pre-Diego releases of Cloud Foundry was replaced by the coordinated actions of the [nsync, Converger, and Cell Reps](../architecture/index.html#nsync-converger). In pre-Diego architecture, the Health Monitor (HM9000) had four core responsibilities:

* Monitor applications to determine their state (e.g. running, stopped, crashed, etc.), version, and number of instances.  HM9000 updates the actual state of an application based on heartbeats and `droplet.exited` messages issued by the DEA running the application.
* Determine applications' expected state, version, and number of instances. HM9000 obtains the desired state of an application from a dump of the Cloud Controller database.
* Reconcile the actual state of applications with their expected state. For instance, if fewer than expected instances are running, HM9000 will instruct the Cloud Controller to start the appropriate number of instances.
* Direct Cloud Controller to take action to correct any discrepancies in the state of applications.

HM9000 was essential to ensuring that apps running on Cloud Foundry remained
available.
HM9000 restarted applications whenever the DEA running an app shut down for any
reason, when Warden killed the app because it violated a quota, or when the
application process exited with a non-zero exit code.

Refer to the [HM9000 readme](https://github.com/cloudfoundry/hm9000) for more
information about the HM9000 architecture.

### <a id='dea-placement'></a>DEA Placement Algorithm &rarr; Diego Auction

In pre-Diego architecture, the Cloud Controller used the [DEA Placement Algorithm](../architecture/cloud-controller.html#dea-placement) to select the host Droplet Execution Agents (DEAs) for application instances that needed hosting.

Diego architecture moves this allocation process out of the Cloud Controller and uses a different algorithm, the [Diego Auction](../diego/diego-auction.html). The Diego Auction prioritizes one-time tasks like staging apps without affecting the uptime of ongoing, running applications like web servers.


### <a id='nats'></a> Message Bus (NATS)

Pre-Diego Cloud Foundry used [NATS](./messaging-nats.html), a lightweight
publish-subscribe and distributed queueing messaging system, for internal
communication between components.

