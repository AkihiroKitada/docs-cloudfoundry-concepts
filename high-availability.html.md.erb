---
title: Scaling Components in CF
owner: Release Integration
---

<strong><%= modified_date %></strong>

This topic describes how to scale the component Virtual Machines (VMs) in a deployment to increase application hosting capacity and decrease downtime during ongoing operation, product updates, and platform upgrades.

Scaling component VMs means changing the number of VM instances dedicated to running a functional component of the system. Scaling usually means increasing this number, while scaling down or scaling back means decreasing it.

For information about scaling applications and maintaining app uptime, see [Scaling an Application Using cf scale](../devguide/deploy-apps/cf-scale.html) and [Using Blue-Green Deployment to Reduce Downtime and Risk](devguide/deploy-apps/blue-green.html).

## <a id='capacity'></a>Scaling Platform Capacity for Applications ##

You can scale platform capacity vertically by adding memory and disk, or horizontally by adding more VMs running instances of Cloud Foundry components.

<%= image_tag("scale_cf.png", :height => "450px", :width => "475px") %>

### <a id='tradeoffs'></a>Trade-offs and Benefits ###

The nature of a particular application should determine whether you scale vertically or horizontally.

**Diego Cells**:

The optimal sizing and CPU/memory balance depends on the performance characteristics of the apps that will run on the Diego Cell. Larger Diego Cells also make for larger points of failure: the system takes longer to rebalance 100 app instances than to rebalance 20 app instances.

**Router**:

Scale the router with the number of incoming requests. In general, this load is much less than the load on Diego Cells.

**Cloud Controller**:

Scale the Cloud Controller with the number of requests to the API and with the number of apps in the system.


## <a id='zero-downtime'></a>High Availability and Zero Downtime ##

During product updates and platform upgrades, the VMs in a deployment restart in succession, rendering them temporarily unavailable. During outages, VMs go down in a less orderly way. Scaling components to a sufficient level of redundancy maintains high availability during both upgrades and outages and can ensure zero downtime.

Deploying or scaling applications to at least two instances per app also helps maintain high availability. See [Scaling an Application Using cf scale](../devguide/deploy-apps/cf-scale.html) for information about scaling applications.

### <a id='processes'></a>Scalable Components

You can scale most Cloud Foundry components to multiple instances to achieve the redundancy required for high availability.

<p class="note"><strong>Note</strong>: Data services may have single points of failure depending on their configuration.</p>

<%= vars.scaling_ert %>

Scale your VMs to the following instance counts:

<table border="1" class="nice">
	<tr>
		<th><strong>Job</strong></th>
		<th><strong>Number</strong></th>
		<th><strong>Notes</strong></th>
	</tr>
	<tr>
		<td>Diego Cell</td>
		<td>&ge; 3</td>
		<td></td>
	</tr>
	<tr>
		<td>Diego Brain</td>
		<td>&ge; 2</td>
		<td>One per AZ, or two if only one AZ.</td>
	</tr>
	<tr>
		<td>Diego BBS</td>
		<td>&ge; 3</td>
		<td>This must be set to an odd number equal to or one greater than the number of AZs you have, in order to maintain quorum.</td>
	</tr>
	<tr>
		<td>Consul</td>
		<td>&ge; 3</td>
		<td>This must be set to an odd number equal to or one greater than the number of AZs you have, in order to maintain quorum.</td>
	</tr>
	<tr>
		<td>MySQL Server</td>
		<td>&ge; 3</td>
		<td>This must be set to an odd number.</td>
	</tr>
	<tr>
		<td>MySQL Proxy</td>
		<td>&ge; 2</td>
		<td></td>
	</tr>
	<tr>
		<td>NATS Server</td>
		<td>&ge; 2</td>
		<td>If you lack the network bandwidth, CPU utilization, or other resources to deploy two stable NATS servers, Pivotal recommends that you use one NATS server.</td>
	</tr>
	<tr>
		<td>Cloud Controller</td>
		<td>&ge; 2</td>
		<td>Cloud Controllers help with API request volume.</td>
	</tr>
	<tr>
		<td>Router</td>
		<td>&ge; 2</td>
		<td>Additional Gorouters help bring more available bandwidth to ingress and egress.</td>
	</tr>
	<tr>
		<td>UAA</td>
		<td>&ge; 2</td>
		<td></td>
	</tr>
	<tr>
		<td>Doppler Server</td>
		<td>&ge; 2</td>
		<td>Deploying additional Doppler servers splits traffic across them. Pivotal recommends to have at least two per Availability Zone.</td>
	</tr>
	<tr>
		<td>Loggregator TC</td>
		<td>&ge; 2</td>
		<td>Deploying additional Loggregator Traffic Controllers allows you to direct traffic to them in a round-robin manner. Pivotal recommends to have at least two per Availability Zone.</td>
	</tr>
	<tr>
		<td>etcd</td>
		<td>&ge; 3</td>
		<td>This must be set to an odd number equal to or one greater than the number of AZs you have, in order to maintain quorum.</td>
	</tr>
</table>

### <a id='single-node'></a>Unscaleable Components ###

Some components only run as a single instance, and cannot be scaled. For these singleton processes, you need to choose a different strategy besides scaling to handle events that degrade availability.

The most important single-node components to address are the HAProxy, NATS, and NFS Server, which affect the platform as a whole. The singleton Collector and Compilation components do not affect platform availability.

**HAProxy**:

Cloud Foundry deploys with a single instance of HAProxy for use in lab and test environments. Production environments should use your own highly-available load balancing solution.

**NATS**:

You might run NATS as a single-node process if you lack the resources to deploy two stable NATS servers.

Cloud Foundry continues to run any apps that are already running even when NATS is unavailable for short periods of time.
The components publishing messages to and consuming messages from NATS are resilient
to NATS failures. As soon as NATS recovers, operations such as health management and
router updates resume and the whole Cloud Foundry system recovers.

Because NATS is deployed by BOSH, the BOSH resurrector will recover the VM if it becomes non-responsive.

**NFS Server**:

For some deployments, an appropriate strategy would be to use your infrastructure's high availability features to immediately recover the VM where the NFS Server runs.
In others, it would be preferable to run a scalable and redundant blobstore service. <%= vars.contact_support %>


### Supporting Component Scaling

####Space####

Ensure that you allocate and maintain enough of the following:

* Free space on Diego Cells so that apps expected to deploy can successfully be staged and run.
* Disk space and memory in your deployment such that if one Diego Cell is down, all instances of apps can be placed on the remaining Diego Cells.
* Free space to handle one AZ going down if deploying in multiple AZs.

####Resource pools####

Configure your <%=vars.pools_link%> according to the requirements of your deployment.

Each IaaS has different ways of limiting resource consumption for scaling VMs. Consult with your IaaS administrator to ensure additional VMs and related resources, like IPs and storage, will be available when scaling.

<%=vars.pcf_pools%>

<%=vars.om_resurrector_header%>
<%=vars.om_resurrector_text%>


### <a id='databases'></a>Databases ###

For scaling internal database components, <%= vars.scaling_ert %>. For database services deployed outside Cloud Foundry, plan to leverage your infrastructure's high availability features and to configure backup and restore where possible.

<%= vars.contact_support %>


