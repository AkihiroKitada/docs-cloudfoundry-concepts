---
title: Configuring CF for High Availability with Redundancy
---

To increase the capacity and availability of the Cloud Foundry platform, and to decrease the chances of downtime, you can scale a deployment up using the strategies described below.

This topic also describes the requirements for a zero downtime deployment. A zero downtime deployment ensures that if individual components go down, your deployment continues to run.

## <a id='zero-downtime'></a>Zero Downtime Deployment ##

This section describes the required configurations for achieving a zero downtime deployment.

####Application Instances####

Deploy at least two instances of every application.

####Components####

Scale your components as described in the [Scaling Platform Availability](#availability) section below. Components should be distributed across two or more <%=vars.az%> (AZs).

####Space####

Ensure that you allocate and maintain enough of the following:

* Free space on DEAs so that apps expected to deploy can successfully be staged and run.
* Disk space and memory in your deployment such that if <%=vars.max_in_flight%> is down, all instances of apps can be placed on the remaining DEAs.
* Free space to handle one AZ going down if deploying in multiple AZs.

####Resource pools####

Configure your <%=vars.pools_link%> according to the requirements of your deployment.

<%=vars.om_resurrector_header%>
<%=vars.om_resurrector_text%>

## <a id='capacity'></a>Scaling Platform Capacity ##

You can scale platform capacity vertically by adding memory and disk, or horizontally by adding more VMs running instances of Cloud Foundry components.

<%= image_tag("scale_cf.png", :height => "450px", :width => "475px") %>

### <a id='tradeoffs'></a>Trade-offs and Benefits ###

The nature of a particular application should determine whether you scale vertically or horizontally.

**DEAs**:

The optimal sizing and CPU/memory balance depends on the performance characteristics of the apps that will run on the DEA.

 * The more DEAs are horizontally scaled, the higher the number of NATS messages the DEAs generate. There are no known limits to the number of DEA nodes in a platform.
 * The denser the DEAs (the more vertically scaled they are), the larger the NATS message volume per DEA, as each message includes details of each app instance running on the DEA.
 * Larger DEAs also make for larger points of failure: the system takes longer to rebalance 100 app instances than to rebalance 20 app instances.

**Router**:

Scale the router with the number of incoming requests. In general, this load is much less than the load on DEA nodes.

**Health Manager**:

The Health Manager works as a failover set, meaning that only one Health Manager is active at a time.
For this reason, you only need to scale the Health Manager to deal with instance failures, not increased deployment size.

**Cloud Controller**:

Scale the Cloud Controller with the number of requests to the API and with the number of apps in the system.

## <a id='availability'></a>Scaling Platform Availability ##

To scale the Cloud Foundry platform for high availability, the actions you take fall into three categories.

 * For components that support multiple instances, increase the number of instances to achieve redundancy.
 * For components that do not support multiple instances, choose a strategy for dealing with events that degrade availability.
 * For database services, plan for and configure backup and restore where possible.

<p class="note"><strong>Note</strong>: Data services may have single points of failure depending on their configuration.</p>

### <a id='processes'></a>Scalable Processes ###

You can think of components that support multiple instances as scalable processes.
If you are already scaling the number of instances of such components to increase platform capacity, you need to scale further to achieve the redundancy required for high availability.

<table border="1" class="nice">
	<tr>
		<th><strong>Component</strong></th>
		<th><strong>Number</strong></th>
		<th><strong>Notes</strong></th>
	</tr>
	<tr>
		<td>Load Balancer</td>
		<td>1</td>
		<td></td>
	</tr>
	<tr>
		<td>MySQL Server</td>
		<td>&ge; 3</td>
		<td>Set this to an odd number to support cluster quorum. For more information about cluster quorum, see <a href='http://docs.pivotal.io/p-mysql/cluster-behavior.html'>Cluster Scaling, Node Failure, and Quorum</a>.</td>
	</tr>
	<tr>
		<td>MySQL Proxy</td>
		<td>&ge; 2</td>
		<td></td>
	</tr>
	<tr>
		<td><a href="./architecture/messaging-nats.html">NATS Server</a></td>
		<td>&ge; 2</td>
		<td>If you lack the network bandwidth, CPU utilization, or other resources to deploy two stable NATS servers, Pivotal recommends that you use one NATS server.</td>
	</tr>
	<tr>
		<td><a href="./architecture/#hm9k">HM9000</a></td>
		<td>&ge; 2</td>
		<td></td>
	</tr>
	<tr>
		<td><a href="./architecture/cloud-controller.html">Cloud Controller</a></td>
		<td>&ge; 2</td>
		<td>More Cloud Controllers help with API request volume.</td>
	</tr>
	<tr>
		<td><a href="./architecture/router.html">Gorouter</a></td>
		<td>&ge; 2</td>
		<td>Additional Gorouters help bring more available bandwidth to ingress and egress.</td>
	</tr>
	<tr>
		<td>Collector</td>
		<td>1</td>
		<td></td>
	</tr>
	<tr>
		<td><a href="./architecture/uaa.html">UAA</a></td>
		<td>&ge; 2</td>
		<td></td>
	</tr>
	<tr>
		<td><a href="./architecture/execution-agent.html">DEA</a></td>
		<td>&ge; 3</td>
		<td>More DEAs add application capacity.</td>
	</tr>
	<tr>
		<td><a href="../devguide/deploy-apps/streaming-logs.html">Doppler Server (formerly Loggregator Server)</a></td>
		<td>&ge; 2</td>
		<td>Deploying additional Doppler servers splits traffic across them.</td>
	</tr>
	<tr>
		<td>Loggregator Traffic Controller</td>
		<td>&ge; 2</td>
		<td>Deploying additional Loggregator Traffic Controllers allows you to direct traffic to them in a round-robin manner.</td>
	</tr>
</table>

### <a id='single-node'></a>Single-node Processes ###

You can think of components that do not support multiple instances as single-node processes.
Since you cannot increase the number of instances of these components, you should choose a different strategy for dealing with events that degrade availability.

First, consider the components whose availability affects the platform as a whole.

**HAProxy**:

Cloud Foundry deploys with a single instance of HAProxy for use in lab and test environments. Production environments should use your own highly-available load balancing solution.

**NATS**:

You might run NATS as a single-node process if you lack the resources to deploy two stable NATS servers.

Cloud Foundry continues to run any apps that are already running even when NATS is unavailable for short periods of time.
The components publishing messages to and consuming messages from NATS are resilient
to NATS failures. As soon as NATS recovers, operations such as health management and
router updates resume and the whole Cloud Foundry system recovers.

Because NATS is deployed by BOSH, the BOSH resurrector will recover the VM if it becomes non-responsive.

**NFS Server**:

For some deployments, an appropriate strategy would be to use your infrastructure's high availability features to immediately recover the VM where the NFS Server runs.
In others, it would be preferable to run a scalable and redundant blobstore service. Contact Pivotal PSO if you need help.

**SAML Login Server**:

Because the Login Server is deployed by BOSH, the BOSH resurrector will recover the VM if it becomes non-responsive.

Secondly, there are components whose availability does not affect that of the platform as a whole. For these, recovery by normal IT procedures should be sufficient even in a high availability Cloud Foundry deployment.

**Syslog**:

An event that degrades availability of the Syslog VM causes a gap in logging, but otherwise Cloud Foundry continues to operate normally.

Because Syslog is deployed by BOSH, the BOSH resurrector will recover the VM if it becomes non-responsive.

**Collector**:

This component is not in the critical path for any operation.

**Compilation**:

This component is active only during platform installation and upgrades.

**Etcd**:

Etcd is a highly-available key value store used for shared configuration and service discovery. You can find more information about running etcd on single nodes in the [etcd GitHub repository](https://github.com/cloudfoundry/etcd#running-a-single-node).

### <a id='databases'></a>Databases ###

For database services deployed outside Cloud Foundry, plan to leverage your infrastructure's high availability features and to configure backup and restore where possible.

Contact Pivotal PSO if you require replicated databases or any assistance.
