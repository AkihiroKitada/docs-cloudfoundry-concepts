---
title: Understanding Container Security
owner: Security
---

<strong><%= modified_date %></strong>

This topic describes how Cloud Foundry (CF) secures the containers that host application instances. For an overview of other CF security features, see the [Understanding Cloud Foundry Security](security.html) topic.

Each instance of an application deployed to CF runs within its own self-contained environment, a [Garden container](./diego/diego-architecture.html#garden) that isolates processes, memory, and the file system. CF isolates all containers from each other using operating system features and the characteristics of the virtual and physical infrastructure where CF is deployed. 

CF mitigates against container breakout and denial of service attacks through the following measures:

* Restricting [traffic](#traffic) to and from containers.
* [Hardening](#harden) all containers by limiting functionality and access rights.
* Assigning application instances to [unprivileged](#unprivileged) or [privileged](#privileged) containers depending on the workload or application type.

## <a id='filesystem'></a>Container Mechanics###

Container isolation is achieved by namespacing kernel resources that would otherwise be shared. The intended level of isolation is set such that multiple containers present on the same host cannot detect each other. Every container includes a private root filesystem – each container has its own Process ID (PID), namespace, network namespace, and mount namespace.

This container filesystem is created by stacking a read-only base filesystem and a container-specific read-write filesystem, commonly known as an overlay filesystem. The read-only filesystem contains the minimal set of operating system packages and Garden-specific modifications common to all containers. Containers can share the same read-only base filesystem because all writes are applied to the read-write filesystem. The read-write filesystem is unique to each container and is created by formatting a large sparse file of a fixed size. This fixed size prevents the read-write filesystem from overflowing into unallocated space.

Resource control is managed using Linux control groups ([cgroups](https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt)) or Windows [job objects](https://msdn.microsoft.com/en-us/library/windows/desktop/ms684161(v=vs.85).aspx). Associating each container with its own cgroup or job object limits the amount of memory that the container may use. Linux cgroups also require the container to use a fair share of CPU compared to the relative CPU share of other containers. 

<p class="note">
    <strong>Note</strong>: BOSH does not support a RedHat Enterprise Linux OS stemcell. This is due to an inherent security issue with the way RedHat handles user namespacing and container isolation.
</p>

## <a id='traffic'></a>Container Network Traffic ##

To isolate applications and control outgoing traffic, each Garden container uses a dedicated virtual network interface (VNI) that consists of a pair of Ethernet addresses: one visible to the application instance running in the container, and the other visible to the host VM’s root namespace. The pair is configured to use IPs in a small and static subnet. Applications are typically allowed to invoke other applications in CF only by leaving the system and re-entering through the load balancer positioned in front of the CF routers. 

### <a id='understand-traffic'></a>Understand Container Networking ###

When an application instance starts, the [Diego cell](./diego/diego-architecture.html#cell-components) (or [DEA](./architecture/execution-agent.html)) on the host VM allocates an IP address and assigns an arbitrary port to the application container. The application uses the `PORT` environment variable provided in the container environment to determine which port to listen on. Because the host assigns a random value to the `PORT` environment variable, the value is generally unique for each application instance.

A host VM has a single IP address. If you configure the deployment with the cluster on a VLAN, as recommended, then all traffic goes through the following levels of network address translation, as shown in the diagram below:

* <strong>Inbound</strong> requests flow from the load balancer through the router to the host cell (or DEA), then into the application container. The router determines which application instance receives each request.

* <strong>Outbound</strong> traffic flows from the application container to the cell (or DEA), then to the gateway on the cell's virtual network interface. This gateway might be a NAT to external networks depending on your IaaS.

  <%= image_tag("images/security/sysbound2.png") %>

### <a id='config-traffic'></a>Configure Network Traffic Rules ###

Administrators can configure rules to govern container network traffic. These rules can prevent system access from external networks and between internal components, as well as restrict applications from establishing connections over the virtual network interface.

Administrators configure these rules at two levels: 

* [Application Security Groups](#asg) apply network traffic rules at the container level.
* [Network Properties](#network-properties) of cells (or DEAs) apply network traffic rules at the host VM level.

#### <a id='asg'></a>Application Security Groups ####

To target applications with specific network traffic rules, <%=vars.app_sec_groups%> (ASGs) define traffic rules for individual containers. The rules specify the protocols, addresses, and ports that are allowed for outgoing traffic. Because they are “allow” rules, their order of evaluation is unimportant when multiple application security groups apply to the same space, org, or deployment. The cell uses these rules to filter and log outbound network traffic.

When applications are first staging, they need traffic rules loose enough to let them pull resources in from across the network. Once they are running, the traffic rules can be more restrictive and secure. To distinguish between these two security needs, administrators can define one ASG for when an application stages, and a different one for when it runs.

To provide granular control in securing a deployment, an administrator can assign ASGs to apply across a CF deployment, or to specific spaces or orgs within a deployment. You define ASGs on the cf CLI using `cf create-security-group` and `cf bind-security-group`. <%=vars.manifest_vs_cli_asg%>

#### <a id='network-properties'></a>Host-level Network Properties ####

Operators can configure the `allow_networks` and `deny_networks` parameters for DEAs to restrict communication between system components and applications. Any Cloud Foundry ASG configurations will overwrite these configurations. For more information, see <%=vars.dea_properties%>.

<p class="note"><strong>Note</strong>: <%=vars.pcf_rec%> recommends that you use Cloud Foundry ASGs to specify egress access rules for your applications. This functionality enables you to more securely restrict application outbound traffic to predefined routes.</p>



## <a id="types"></a> Cloud Foundry Container Types

CF uses [Garden](./diego/diego-architecture.html#garden) to manage the containers that host instances of the applications running on a deployment. Garden has two container types: unprivileged and privileged. CF determines which container type to use depending on the workload or application type.

### <a id="unprivileged"></a> Unprivileged Containers

Docker images come with their own root filesystem and user. CF cannot trust the root filesystem and cannot assume that the container user process will never be root. For that reason, CF runs applications based on Docker images in unprivileged containers.

See [Using Docker in Cloud Foundry](docker.html) for more information.

### <a id="privileged"></a> Privileged Containers

CF runs buildpack applications, as well as [staging tasks](how-applications-) for both buildpack and Docker image applications, in containers that use the cflinuxfs2 root filesystem. Because all processes invoked in these containers are run as the unprivileged user `vcap`, CF runs the containers as privileged.

## <a id="hardening"></a> Container Hardening

Cloud Foundry does the following to mitigate container breakout and denial of service attacks:

FIRST -- CF does this to ALL CONTAINERS.

Then break down the privileged vs unprivileged containers.


### <a id="namespaces"></a> Namespaces

A namespace wraps a global system resource in an abstraction that makes it appear to the processes within the namespace that they have their own isolated instance of the global resource. Changes to the global resource are visible to other processes that are members of the namespace, but are invisible to other processes. One use of namespaces is to implement containers.

Linux provides the following namespaces:

<table id='namespaces' border='1' class='nice'>
	<tr>
		<th>Namespace</th>
		<th>Isolates</th>
	</tr><tr>
		<td>IPC</td>
		<td>System V IPC, POSIX message queues</td>	
	</tr><tr>	
		<td>Network</td>
		<td>Network devices, stacks, ports, etc.</td>
	</tr><tr>
		<td>Mount</td>
		<td>Mount points</td>
	</tr><tr>
		<td>PID</td>
		<td>Process IDs</td>
	</tr><tr>
		<td>User</td>
		<td>User and group IDs</td>
	</tr><tr>
		<td>UTS</td>
		<td>Hostname and NIS domain name</td>
	</tr>
</table>

CF uses the full set of linux namespaces (PID, Mount, Network, IPC, UTS, User) to provide isolation between containers running on the same host.
	<p class="note"><strong>Note</strong>: The User namespace is not used for privileged containers.</p>

In unprivileged containers, CF maps the root UID/GID (0) inside the container user namespace to another UID/GID on the host to prevent an application from inheriting the root UID/GID on the host if it breaks out of the container.

* This is the same UID/GID for all containers.
* All UIDs apart from UID 0 are mapped to themselves. UID 0 inside the container namespace is mapped to MAX_UID-1 outside of the container namespace.
* Container Root does not grant Host Root permissions

### <a id="capabilities"></a> Capabilities

For permission checks, traditional UNIX implementations distinguish two categories of processes: 

* **privileged processes** 
	* Effective UID is 0, referred to as superuser or root 
	* bypasses all kernel permission checks
* **unprivileged processes**
	* Effective UID is nonzero
	* subject to full permission checking based on the process credentials, such as effective UID, effective GID, and supplementary group list

Linux divides superuser privileges into distinct units, known as capabilities, which you can enable and disable independently. Capabilities are a per-thread attribute.

CF limits root user privileges in unprivileged containers by dropping the following capabilities from all container processes:

* CAP\_DAC\_READ\_SEARCH
* CAP\_LINUX\_IMMUTABLE
* CAP\_NET\_BROADCAST
* CAP\_NET\_ADMIN
* CAP\_IPC\_LOCK
* CAP\_IPC\_OWNER
* CAP\_SYS\_MODULE
* CAP\_SYS\_RAWIO
* CAP\_SYS\_PTRACE
* CAP\_SYS\_PACCT
* CAP\_SYS\_BOOT
* CAP\_SYS\_NICE
* CAP\_SYS\_RESOURCE
* CAP\_SYS\_TIME
* CAP\_SYS\_TTY\_CONFIG
* CAP\_LEASE
* CAP\_AUDIT\_CONTROL
* CAP\_MAC\_OVERRIDE
* CAP\_MAC\_ADMIN
* CAP\_SYSLOG
* CAP\_WAKE\_ALARM
* CAP\_BLOCK\_SUSPEND

Additionally, we also drop CAP\_SYS\_ADMIN for unprivileged containers.

CF mounts /proc and /sys read-only inside the container
Limit access to devices using cgroups, explicitly whitelist safe device nodes.
	* /dev/full
	* /dev/fuse
	* /dev/null
	* /dev/ptmx
	* /dev/pts/*
	* /dev/random
	* /dev/tty
	* /dev/tty0
	* /dev/tty1
	* /dev/urandom
	* /dev/zero
	* /dev/tap
	* /dev/tun
* Disallow dmesg access for unprivileged users and all users in unprivileged containers.
* Use a chroot when importing docker images from docker registries.
* Establish a container-specific overlay filesystem mount and pivot_root into that overlay to isolate the container from the host system’s filesystem.
* Eliminate any dependencies on scripts and binaries inside the rootfs, by not calling any binary or script inside the container filesystem.
* Avoid side-loading binaries in the container (via bind mounts or other methods), and simply “re-exec” the same binary (read from /proc/self/exe) any time we need to run something in a container.
* Establish a virtual ethernet pair for each container for its network traffic.
	* One interface in the pair is inside the container’s network namespace, and is the only non-loopback interface accessible inside the container.
	* The other interface remains in the host network namespace and is bridged to the container-side interface.
	* Egress whitelist rules are applied to these interfaces according to Administrator configured policy.
	* First-packet logging rules may also be enabled on TCP whitelist rules.
	* DNAT rules are established on the host to enable traffic ingress from the host interface to whitelisted ports on the container-side interface.
* Apply disk quotas by confining container-specific filesystem layers to loop devices with the specified disk-quota capacity.
* Apply a total memory usage quota via the memory cgroup and destroy the container if the memory usage exceeds the quota.
* Apply a fair-use limit to CPU usage for processes inside the container via the cpu.shares group.
